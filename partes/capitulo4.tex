\chapter{Implementación}

Una vez descrita la arquitectura modular del sistema y las funcionalidades asociadas a los diferentes componentes del sistema, en este capítulo se detallarán las cuestiones más relevantes en cuanto a su implementación.

Así, en primer lugar, se describirán las funciones correspondientes al ... .

A continuación se explicará las estructuras de datos utilizadas y como se realizó la implementación de cada uno de los módulos mencionados anteriormente haciendo uso de las herramientas presentadas por el lenguaje de scripting de Bro.

\label{capitulo4}
\lhead{Capítulo 4. \emph{Detalles de implementación}}
\section{Implementación del modulo de segmentación}

En esta sección, se explicará de manera detallada la implementación del módulo de segmentación y las estructuras de datos utilizadas en el mismo. Este módulo forma parte de los tres que conforman el sistema de detección de intrusiones basado en SSM. 

Estos segmentos obtenidos luego ser almacenados en una estructura de datos global para que de esta forma, la información obtenida pueda ser utilizada por el resto de los módulos.

La implementación de la función de normalización que se explicará a continuación está basada en el uso de una hash table con todos los elementos de una encoding table de tipo UTF-8. Por otra parte, la función de segmentación que teóricamente esta basada en un autómata que reconoce el lenguaje de los URI se implementará tomando la inspiración de una gramática que genere el mismo lenguaje que reconoce el autómata (el lenguaje de los URIs). Esto se va a realizar de esta manera ya que las herramientas presentadas por el lenguaje de scripting de Bro facilitan la implementación de un segmentador inspirado en una gramática.

\subsection{Estructura de datos}
En el módulo de segmentación existen dos estructuras de datos fundamentales. Una es utilizada en la función de normalización y la otra en la de segmentación.

\subsubsection{Normalización}
\label{sssec:estructuraNormalizacion}

La estructura de datos utilizada en la función de normalización es un hash table que contiene los elementos de un encoding table de tipo UTF-8, en donde las claves de la tabla serían los elementos de tipo UTF-8 y los atributos de las mismas corresponderían a los caracteres sin alguna codificación. Con esta tabla, se quiere implementar un diccionario que mapee los elementos de tipo UTF-8 con sus respectivos caracteres lo mas rapido posible.

Se utilizó un hash table para implementar este diccionario debido a que es ampliamente conocido que este tipo de tablas propocionan mucha eficiencia en el tiempo de búsqueda de sus elementos, como lo explica Cormen en su libro: "Introduction to Algoriths":

“Una hash table es una estructura de datos efectiva para implementar diccionarios. A pesar de que la búsqueda de un elemento en una hash table puede tomar el mismo tiempo de búsqueda que una lista enlazada - O(n)/tiempo en el peor de los casos - en la práctica, el mapeo funciona extremadamente bien.  Bajos suposiciones razonable, el tiempo de búsqueda promedio de una hash table es de O(1)”.

Para implementar este tipo de estructura se utilizó el tipo de dato “table” otorgado por el lenguaje de scripting de Bro. Un ejemplo concreto de como luce la estructura de dato implementada en Bro se mostrará a continuación:

\begin{verbatim}
global encoding : table[string] of string = {   ["%21"] =    "!"     ,
                                                ["%22"] =    "”"    ,                                        
                                                ["%23"] =    "#"    ,
                                                ["%24"] =    "$"    }
\end{verbatim}

La funcionalidad de esta tabla de codificación será explicada en la sección \ref{subsec:implementacionNorm}.

\subsubsection{Función de segmentación}
\label{sssec:estructuraSegmentacion}

Por otra parte, la estructura de datos utilizada en la función de segmentación de este módulo es un registro en el cual se almacenan todos los segmentos que se originan a partir de la segmentación de un URI, el URI sin segmentar, un booleano que informa si el URI segmentado sigue con la sintaxis establecida en el RFC 3896, y el número de estados que fueron visitados en el autómata presentado en el modelo teórico.

En Bro existen las palabras claves “record” y “type” y se utilizan de forma similar en las que se emplean las palabras claves “typedef” y “struct” en C. Con estas palabras, Bro permite combinar nuevos tipos de datos y crear tipos de datos compuestos para adaptarse a las necesidades de la situación. 

Se escogió un registro para almacenar la información ya que el lenguaje de scripting de Bro solo presenta como alternativa el tipo de dato “record” para crear un tipo de dato compuesto. 

El registro creado esta conformado por los siguientes campos:
\begin{itemize}
\item uri: Es una variable de tipo string que almacena el URI sin segmentar. Este campo del registro será utilizado por el módulo de entrenamiento al momento de escribir los logs correspondientes.
\item host: Es una variable de tipo hash table cuyas claves son número y sus valores  son de tipo string. En esta sección se almacenan los segmentos del URI correspondiente al host.
\item path: Es una variable de tipo hash table cuyas claves son número y sus valores  son de tipo string. En esta sección se almacenan los segmentos del URI correspondiente al path.
\item query: Es una variable de tipo hash table cuyas claves y sus valores son de tipo string. En esta sección se almacenan los atributos y los valores del query del URI en caso de que el mismo posea. Los atributos y los valores se almacenarán en una hash table como se mencionó con anterioridad, en donde la clave del mismo serán los atributos del query y en donde los valores de las misma serán los valores del query del URI.
\item fragment: Es una variable de tipo string que almacenará el fragment del URI en caso de poseerlo.
\item número de estados: es una variable de tipo entero que almacena el número de estados del autómata que se emplea en el módelo teórico fueron visitados para reconocer el URI. Este campo del registro será utilizado por el módulo de evaluación.
\item uri correcto: es una variable de tipo booleano que indica si el URI está sintácticamente correcto o no.
\end{itemize}

En Bro, esta estructura estaría escrita de la siguiente forma:

El uso de esta estructura y el de cada uno de sus campos se explicara a detalle en las sección \ref{subsec:implementacionAutom}.

Además de utilizar el tipo de dato “uriSegmentado”, el módulo de segmentación hace uso de una estructura de datos compuesta, primitiva del lenguaje de scripting de Bro llamada  “URI”. Dicha estructura posee los siguientes campos:

\begin{itemize}
\item scheme: Es un campo opcional de tipo “string” que almacena el protocolo del Uri.

\item netlocation: Es un campo de tipo “string” que almacena el nombre de dominio o la dirección IP del Uri.

\item portnum: Es un campo opcional de tipo “count”  que almacena el número de puerto del uri.
path:Es un campo de tipo “string” que almacena la ruta del uri.

\item file\_name: En un campo opcional de tipo “string” que almacena el nombre completo  del archivo de la ruta del uri junto a su extensión.

\item file\_base: En un campo opcional de tipo “string” que almacena el nombre  del archivo de la ruta del uri sin su extensión.

\item file\_ext: En un campo opcional de tipo “string” que almacena la extensión del archivo de la ruta.

\item params: Es un campo opcional de tipo “table” que almacena todos los parámetros de la consulta del uri. Esta tabla mapea todos los atributos con sus valores.
\end{itemize}

La estructura "URI" se utilizará al momento de segmentar las consultas del URI.

\subsection{Implementación del segmentador y el analizador sintáctico}\label{subsec:implementacionAutom}

En la función de segmentación, el problema que se quiere resolver es el siguiente: guardar los segmentos de un URI delimitados por los delimitadores presentados en la sección ~\ref{sec:delimitadores} en la estructura de datos de tipo “uriSegmentado”, detallada en la sección ~\ref{sssec:estructuraSegmentacion}, y a la vez verificar que el mismo tenga una estructura sintáctica correcta según los estándares del RFC 3986. En las base teórica, este problema es resuelto  mediante un autómata que se encarga de reconocer y evaluar en cada uno de sus estados la probabilidad de generación de cada uno de los segmentos. 

En esta sección se explicará la forma en que se modeló y se implementó el problema presentado con anterioridad haciendo uso de las herramientas presentadas por el lenguaje de scripting de Bro.

Estos tokens utilizarán las expresiones regulares que soporta Bro. Estas están basadas en las expresiones regulares empleadas en Lex, la librería para realizar análisis léxicos del lenguaje de programación C.

La lista de tokens utiliza es la siguiente:

\begin{itemize}
\item protocolo: “http”|”https”
\item host: $(([a-z]+[a-z0-9\-]*[.])?([a-z0-9]+[a-z0-9\-]*[.])+[a-z]{2,3}|localhost)(:([0-9]{1,5}))?|$\\ ((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(:([0-9]{1,5}))?
\item elementoPath: [\^?\#]*
\item atributo: [\^\#\&=]
\item valor: [\^\#\&]
\item fragmento: (\#.*)
\end{itemize}

Por otra parte, la gramática libre de contexto en la que está basada la función de análisis sintáctico es la siguiente.

\begin{equation}\label{eq:gramatica}
\begin{aligned}
S -> protocolo “ :// ” H  \\
H -> host “/” P | host \\
P -> P’ “?” Q | P’ “\#” F | P’ \\
Q -> Q’ “\#“ F | Q’ \\
F -> fragment \\
P’ -> \lambda \\
P’ ->  elementoPath \\
P’ -> elementoPath “/” P’ \\
Q’ -> atributo “=” valor \\
Q’ -> atributo “=” valor “\&” Q’ \\
\end{aligned}
\end{equation}

Donde, el elemento de inicio de esta gramática es el símbolo no terminal “S”

Para almacenar los segmentos en la estructura de datos compuesta de tipo “uriSegmentado”" que se presentó en la sección ~\ref{sssec:estructuraSegmentacion} a la gramática \ref{eq:gramatica} se le van a asociar atributos a algunas de las reglas de la misma. A este tipo de gramática se le llama gramática de atributos. Los atributos de dicha gramática serán escritos en el lenguaje de scripting de Bro.

\begin{equation}\label{eq:gramaticaAtributos}
\begin{aligned}
S -> protocolo “ :// ” H { uri\$protocolo := protocolo } \\
H -> host “/” P | host { uri\$host := host  } \\
P -> P’ “?” Q | P’ “\#” F | P’ \\
Q -> Q’ “\#“ F | Q’ \\
F -> fragment { uri\$fragment := fragment } \\
P’ -> \lambda  { uri\$path.append(“”) } \\
P’ ->  elementoPath { uri\$path.append(elementoPath) } \\
P’ -> elementoPath “/” P’ { uri\$path.append(elementoPath) } \\
Q’ -> atributo “=” valor { uri\$query[atributo] := valor } \\
Q’ -> atributo “=” valor “\&” Q’ { uri\$query[atributo] := valor } \\
\end{aligned}
\end{equation}

    La gramática de atributos presentada \ref{eq:gramaticaAtributos} representa el modelo de lo que se implementó haciendo uso de Bro.

    Como Bro no cuenta con una librería propia para programar un parser de manera sencilla, se hizo uso de las herramientas con las que cuenta este para procesar cadena de caracteres y expresiones regulares. Las funciones fundamentales que se utilizaron para implementar  la gramática \ref{eq:gramaticaAtributos} fueron: “split”, “split\_all” y "decompose\_uri".

A continuación se explicara un poco el funcionamiento de cada una de ellas.

\begin{itemize}
\item split:
La función “split” tiene la siguiente forma
function(str: string, re: pattern) : string\_array Attributes:\&deprecated

Esta función se encarga de dividir una cadena de caracteres de acuerdo a un patrón e introduce el resultado en un arreglo. Por ejemplo, $split"a-b--cd", /(\-)+/)$ retorna $\{"a", "b", "cd"\}.$

Los parámetros de dicha función son:

\begin{itemize}
\item Str:La cadena de caracteres que se quiere dividir. 
\item Re: El patrón que describe lo delimitadores mediante los cuales será dividida la cadena de caracteres. 
\item Retorna: Un arreglo de caracteres donde cada elemento corresponde a una subcadena de caracteres de Str separado por Re.
\end{itemize}

\item split\_all():
Esta función realiza el mismo trabajo que “split” con la diferencia que los separadores son incluidos también el parámetro de salida.  Por ejemplo, $split\_all("a-b--cd", /(\-)+/)$ retorna $\{"a", "-", "b", "--", "cd"\}$.

\item decompose\_uri:
La función decompose\_uri tiene la siguiente forma: function(uri: string) : URI

decompose\_uri dado un URI, retorna una estructura de datos compuesta de tipo URI explicada en la sección ~\ref{sssec:estructuraSegmentacion} que contiene información como el protocolo,la ruta, el número de puerto y los parámetros de las consultas del URI recibido.
\end{itemize}

Para implementar la gramática ~\ref{eq:gramaticaAtributos} se modelaron los elementos no finales de la misma como funciones. Por otra parte, los elementos finales, fueron extraídos haciendo uso de “split” y “split\_all”.

A continuación, se mostrará un pseudocódigo en donde se podrá apreciar de manera más clara el procedimiento llevado a cabo para implementar la grámatica \ref{eq:gramaticaAtributos} haciendo uso de las funciones nombradas con anterioridad.

\begin{verbatim}
# Hace el trabajo que realiza el símbolo no terminal "S" de la gramática
function inicio(url: string){
    # Se parsea el protocolo
    local test_pattern = /(http(s)?:\/\/)?/;
    local results = split(uri,test_pattern);
    Se verifica la correctitud sintáctica
    Si hay correctitud sintáctica entonces, se llama a la función parseHost
}

# Hace el trabajo que realiza el símbolo no terminal "H" de la gramática
function parseHost(url: string){
    # Se parsea el host
    local test_pattern = /(([a-z]+[a-z0-9\-]*[.])?([a-z0-9]+[a-z0-9\-]*[.])\\
                          +[a-z]{2,3}|localhost)(:([0-9]{1,5}))?/;
    local results = split_all(url, test_pattern);
    Se verifica la correctitud sintáctica
    Si hay correctitud sintáctica entonces:
        Se almacena el host en uri$host
        Si existe una ruta entonces llamaremos a la función parsePath
}

# Hace el trabajo que realiza el símbolo no terminal "P" de la gramática
function parsePath(url: string){
    # Se parsea la ruta
    local test_pattern = /[^?#]*\/?/;
    local results = split_all(urlResult, test_pattern);
    Se verifica la correctitud sintáctica
    Si hay correctitud sintáctica entonces:
        Se llama a la función segmentarQuery
        Si existe un query, entonces llamaremos a la función parseQuery
        Si no existe un query, pero si un fragment, 
        entonces llamaremos a la función parseFragment

}

# Hace el trabajo que realiza el símbolo no terminal " P' " de la gramática
function segmentarPath(path: string){

    local test_pattern = /\//;
    local results = split(url, test_pattern);
    parsedUri$path = results;
}

# Hace el trabajo que realiza el símbolo no terminal "Q" de la gramática
function parseQuery(url: string){
    # Se parsea el query
    local test_pattern = /\?[^#&]+((=[^#&]*)?(&[^#&]+(=[^#&]*)?)*)?/;
    local results = split_all(url, test_pattern);
    Se verifica la correctitud sintáctica
    Si hay correctitud sintáctica entonces:
        Se llama a la función segmentarQuery
        Si existe un fragment, entonces llamaremos a 
        la función parseFragment

}

# Hace el trabajo que realiza el símbolo no terminal " Q' " de la gramática
function segmentarQuery(query: string){

    local queryUri : URI;
    # Se segmenta el query string.
    queryUri = decompose_uri(query);
    parsedUri$query = queryUri$params;
}

# Hace el trabajo que realiza el símbolo no terminal "F" de la gramática
function parseFragment(url: string){
    # Se parsea la ruta
    local test_pattern = /#.*/;
    local results = split_all(url, test_pattern);
    Se verifica la correctitud sintáctica
    Si hay correctitud sintáctica entonces:
        Se almacena el fragment en uri$fragment
}
\end{verbatim}

    Entonces, si se quiere poner en funcionamiento el módulo para segmentar de deberá llamar a la función “inicio” con un URL como parámetro de entrada.

\subsection{Implementación del la normalización de los URIs}\label{subsec:implementacionNorm}

La normalización de los URI debe realizarse de acuerdo a las especificaciones establecidas en la Sección 3.X, habiéndose implementado para ello una función denominada X, con el siguiente formato.

\textbf{La normalización de los URI debe realizarse de acuerdo a las especificaciones establecidas en la Sección 3.X, habiéndose implementado para ello una función denominada X, con el siguiente formato}

Esta función tomará como entrada una variable de tipo string y mediante un "loop for" se itera por cada una de las claves de la tabla que se presentó en la seccón ~\ref{sssec:estructuraNormalizacion}. Si alguno de los elementos de la tabla está contenido en el string que la función recibe como entrada, entonces se procederá a reemplazar el elemento del párametro de entrada por el valor que posee dicha clave en la hash table. Esta tarea de subsitución se realizará haciendo uso de la función “subst\_string” que proporciona Bro.

“subst\_string” es una función de la forma: function(s: string, from: string, to: string) : string

Se encarga de hacer sustituciones en una cadena de caracteres. Los parámetros de esta son:

\begin{itemize}
\item S: Cadena de caracteres en la que se efectúa la sustitución.
\item From: La cadena de caracteres que se va a buscar en “S” para ser sustituida.
\item To: Cadena de caracteres que pasará a sustituir a “From”.
\end{itemize}

A continuación se presenta la implementación de dicha función haciendo uso del lenguaje de scripting de Bro.

\begin{verbatim}

function normalizarUri(url: string): string {
    for (word in encoding){

        if (word in urlFinal){
            urlFinal = subst_string(urlFinal,word,encoding[word]);

        }
}
\end{verbatim}

Donde "encoding" es un tabla como la que se presentó en la sección ~\ref{sssec:estructuraNormalizacion}
        
\section{Implementación del modulo de evaluación}
En esta sección se explicará de manera detallada las estructuras de datos utilizadas y la manera en la que se implementó el módulo de evaluación explicado en la sección \ref{sec:evaluacion}  haciendo uso del lenguaje de scripting de Bro. 


\subsection{Estructura de datos}

El módulo de evaluación del sistema cuenta con varias estructura de datos compuestas de tipo “register” y de tipo “table”. La funcionalidad de leer el modelo de normalidad consta de dos estructuras de tipo “table” y dos de tipo “register”, mientras que la que se encarga de calcular el índice de normalidad cuenta con una sola estructura de tipo “table”.

\subsubsection{Modelo de normalidad}
\label{sssec:estructuraModelo}

Existen dos estructuras de tipo “register” en la función para leer el modelo de normalidad y dos de tipo “table”.
Las estructuras de tipo “register” poseen los siguientes campo:

La primera, cuyo nombre es “Word” posee los campos “word” y  “state”.

\begin{itemize}
\item “word” es un campo de tipo string en donde se almacenarán las palabras del vocabulario de cada uno de los estados del autómata presentado en la imagen \ref{fig:ssm}.
\item “state” es un campo de tipo string en donde se indicará el estado del autómata presentado en la imagen \ref{fig:ssm} al que pertenece la palabra almacenada en el campo “word”.
\end{itemize}

En Bro, esta estructura sería escrita de la siguiente forma:

\begin{verbatim}
    type Word: record {
            state: string;
            word: string;
    };
\end{verbatim}

La segunda estructura llamada “Probability” solo posee un campo llamado “probability”.

\begin{itemize}
\item “probability” es un campo de tipo “double” en donde se almacenará la probabilidad de generación de las palabras que se encuentran en el modelo.
\end{itemize}

La forma de esta estructura en el lenguaje de scripting de Bro sería la siguiente:
\begin{verbatim}
    type Probability: record {
            probability: double;
    };
\end{verbatim}

Por otra parte, las estructuras de tipo “table” son dos. Una tabla será utilizada para almacenar las palabras del vocabulario, la probabilidad de generación y el nombre del estado al que pertenece dicha información mientras que la otra almacenará las probabilidades de fuera de vocabulario de cada uno de los estados y el valor del parámetro $\theta$.

La primera tabla es una hash table que posee dos elementos tipo “string” como clave, y como valor tiene un campo de tipo “Probability”. Un ejemplo de como luciría dicha estructura de datos en Bro seria la siguiente:

\begin{verbatim}
Btable: table[string,string] of Evaluacion::Probability;
\end{verbatim}

“Btable” almacenará del modelo de normalidad del sistema, es decir, las palabras del vocabulario, la probabilidad de generación de las mismas y el estado al que pertenece dicha información. Las claves de esta tabla serán las palabras del vocabulario y el nombre del estado al que pertenecen. Toda esta informacion será dada a través de un archivo de texto cuya estructura será explicada en las sección ~\ref{sec:lecturaModelo}.

La segunda tabla es una hash table que tendrá solo un elemento como clave de tipo “string”. Los valores de la misma serán campo tipo “Valor”. En Bro, dicha estructura luciria de la siguiente manera.

\begin{verbatim}
config : table[string] of Evaluacion::Valor;
\end{verbatim}

En la tabla “config” como se mencionó anteriormente se van a almacenar las probabilidades de fuera de vocabulario de cada uno de los estados del autómata y el parámetro $\theta$. La clave de la misma sería una cadena de caracteres que identifique cada una de las probabilidades  y el parámetro “theta”. Esas etiquetas serían las siguientes: Poov1, Poov2, Poov3, Poov4, Theta.  

Los campos “word”, “state” y “probability” no se almacenaron en una sola estructura ya que para leer el modelo de normalidad que estará almacenado en un archivo de texto se hizo uso de una herramienta otorgada por Bro que requiere que exista una estructura de datos  para las claves de la tabla que almacenará la información (en este caso “Btable”) y otra para los valores de la misma.

\subsubsection{Evaluación de las probabilidades de los URI}
\label{sssec:estructuraEvaluacion}

La función  que se encarga de calcular el índice de anormalidad  y evaluar si el mismo es anómalo o no, solo contiene una estructura compuesta de tipo “record” y se utiliza junto a una herramienta de Bro que funciona para escribir logs. Los logs que son escritos a través de esta herramienta son archivos de texto que contiene una lista de los URI que presentan anomalías.

La estructura lleva como nombre “InfoAtaque” y contiene los siguientes campos:

\begin{itemize}
\item clasificacion: es un campo de tipo “string” que almacena la clasificación de los URIs anómalos, es decir, aquí se indica si el mismo presenta anomalía por estar sintácticamente mal construidos o porque el índice de anormalidad sobrepasó el parámetro $\theta$.
\item uri: es un campo de tipo “string” en el que se va a almacenar el uri que va a ser registrado en el log.
\item probability: es un campo de tipo “string” en donde se va a almacenar el valor del índice de anormalidad del uri.
\end{itemize}

Todos los campos anteriormente descritos poseen la cualidad de ser de tipo “log” tambien. Con esto se indica cuál de los elementos de la estructura de datos se escribirá sobre los logs.

    El registro “InfoAtaque” se definiría de la siguiente manera haciendo uso del lenguaje de scripting de Bro:

\begin{verbatim}
    type InfoAtaque: record {
                clasificacion: string &log &default = "";
                uri : string &log &default = "";
                probability: string &log &default = "";
    }; 
\end{verbatim}

\subsection{Modelo de normalidad en Bro}
\label{sec:lecturaModelo}
En esta sección se explicará cual es el formato de almacenamiento del modelo de normalidad y como se implementó la lectura del mismo. El modelo teórico que fue explicado en la sección ~\ref{sec:modeloSSM} posee elementos como: un conjunto “S” de estados, un conjunto “O” de símbolos observables que se encuentran en cada estado, una matriz “A” que contiene la probabilidad de transición entre estados, un conjunto “B” de vectores que contiene la probabilidad de las palabras observadas en cada estado y un vector de probabilidades iniciales. No obstante, los únicos elementos del modelo que se necesitan ingresar en sistema para que este funcione y realice las tareas de evaluación son: el conjunto “O” de símbolos observables que se encuentra en cada estado y el conjunto de vectores “B”. El conjunto “B” es necesario para resolver la expresión \ref{eq:sumB}. Por otra parte, es necesario hacer uso tanto de “B” como “O” para obtener el valor de $b_{qtot}$ presentado en la ecuación \ref{eq:Pqtot}. Este elemento es sumamente importante para calcular el índice de anormalidad. Por otra parte, será necesario introducir al sistema los valores de probabilidad de fuera de vocabulario (Poov), ya que son necesarios para obtener el valor de $p_{qtot}$ en caso una cadena de caracteres del URI que se esté analizando no se encuentre en el conjunto de observaciones “O”. Además, se requiere el valor del parámetro $\theta$ para evaluar el índice de anormalidad según lo estipulado en la ecuación \ref{eq:ClaseU} .

    Entonces, en conclusión, los parámetros que requieren ser introducidos al sistema de detección de intrusiones para que este funcione de manera correcta son: el conjunto de observaciones de cada estado (O), el conjunto de vectores de probabilidad de las palabras observadas en cada estado ( $B_{S}, B_{P}, B_{A}, B_{V}$ ), los valores de probabilidad de fuera de vocabulario, es decir $P_{oovS}, P_{oovP}, P_{oovA}, P_{oovV}$ y el valor del parámetro $\theta$.

    *FORMATO*

Estos parámetros serán introducidos a través de dos archivos, un archivo llamado “config” que contendrá los valores de la probabilidad de fuera de vocabulario y el valor del parámetro $\theta$ y otro llamado “modeloBro.log” en el cual estará el conjunto de observaciones de cada estado junto a sus probabilidades de generación.

Tanto el archivo “config” como el “modeloBro.log” están regidos bajo un formato que establece la herramienta de Bro para leer archivos de entrada. Esta decisión fue tomada para poder utilizar dicha herramienta y así facilitar la lectura de ambos archivos.

El formato que establece Bro para los archivos de entrada define que la información se debe introducir en columnas separada mediante tabs, y que debe existir un encabezado al inicio del mismo que inicie abriendo con un numeral (\#) seguido de la palabra “fields”. Luego de esta palabra se escribirán los nombres que se le asignará a cada columna del archivo.

El formato del archivo “config” será el siguiente:

\begin{verbatim}
         #fields    clave    valor
         Poov1    0.0001
         Poov2    0.0001
         Poov3    0.0001
         Poov4    0.0001
         Theta    12.0
\end{verbatim}

La primera columna corresponde a las etiquetas que identifican los valores que se encuentran en la segunda columna. Es importante recalcar que los nombres “Poov1”, “Poov2”, “Poov3”, “Poov4” y “Theta” deben ser escritos en el archivo obligatoriamente de la misma forma en la que aparecen en el ejemplo.

\# Dar un ejemplo de como se almacena la información
La información que se encuentre en el archivo “config” será almacenada en  la tabla llamada “config” que fue explicada en la sección \ref{sssec:estructuraModelo}. También es importante recalcar que los nombres de las columnas (“clave” y “valor” en este caso) deben corresponder con los nombres de los atributos de las estructuras de datos de tipo “register” llamada “Clave” y “Valor” presentada en la sección \ref{sssec:estructuraModelo}

Por otro lado, el formato del archivo “modeloBro.log” es el siguiente:

\begin{verbatim}
      #fields    state    word    probability
      #types    string    string    double
      Bss    192.168.1.13    1.0
      Bsp    idr    0.16759
      Bsa    idpd    0.000006
      Bsv    nombre    0.000175
      numeroPalabraSs    numTotal    205069.0
      numeroPalabraSp    numTotal    423870.0
      numeroPalabraSa    numTotal    319727.0
      numeroPalabraSv    numTotal    319727.0
\end{verbatim}

La segunda columna de este archivo (“word”) corresponde al conjunto de observaciones “O” de cada uno de los estados, mientras que la tercera columna (“probability”) indica la probabilidad de generación de cada palabra, es decir, esta columna almacena los datos de los vectores $B_{S}, B_{P}, B_{A}, B_{V}$. Por otra parte, la primera columna (“state”) indica a que estado del autómata pertenece cada palabra y su probabilidad de generación.

Las filas del archivo “modeloBro.log” que tienen en la primera columna las palabras:       “numeroPalabraSs”, “numeroPalabraSp”, “numeroPalabraSa”, “numeroPalabraSv” contienen el número de palabras que han sido procesadas por estado. Esta informacion será utilizada en el entrenamiento en modo offline que será explicado en la sección \ref{sec:entrenamientoOffline}.

*MODO EN EL QUE SE LEEN LOS ARCHIVOS*

Una vez explicado el formato de los archivos que contienen los elementos del modelo y los parámetros necesarios para realizar el cálculo y la evaluación del índice de normalidad de los URIs a evaluar se procederá a explicar el funcionamiento de las funciones encargadas de leer los archivos con la información de entrada.

Para leer dicha informacion se hizo uso del  “input framework” que otorga Bro como herramienta para leer archivos de entrada y guardarla en tablas para que la misma pueda ser accedida de manera mas rapida y comoda dentro del código del programa. 

La función que se encarga de leer y almacenar en una tabla la información que está dentro de los archivos “config” y “modeloBro.log”  se llama $“add_table”$ y pertenece al módulo llamado “Input”.


%Extraido de : https://www.bro.org/sphinx/frameworks/input.html, https://www.bro.org/sphinx/scripts/base/frameworks/input/main.bro.html\#id-Input::add_table, https://www.bro.org/sphinx/scripts/base/frameworks/input/main.bro.html\#type-Input::TableDescription 
%
La misma tiene la siguiente forma: $Input::add_table(description: Input::TableDescription) : bool$ 

Donde $"Input::TableDescription"$ es una estructura de datos tipo “record” cuyos campos utilizados fueron los siguientes:

\begin{itemize}
\item source: campo de tipo “string” que almacena el nombre del archivo que va a ser leído.
\item name: campo de tipo “string” que almacenará el nombre el nombre que se le asignará al flujo de entrada.
\item destination: Nombre de la tabla que almacena la información contenida en los archivos.
\item idx: Nombre del registro que definirá los valores que utilizará la tabla que almacena la información del archivo como clave.
\item val: Campo opcional que almacena el nombre del registro que define los valores de la tabla que almacena la información de los archivos de entrada.
\end{itemize}

Un ejemplo concreto de cómo luciría esta función si se quieren leer los archivos “config” y “modeloBro.log” seria la siguiente:

La lectura del archivo “config” sería la siguiente:

\begin{verbatim}
Input::add_table([\$source=”config”, \$name=”config”,
                          \$idx=Clave, \$val=Valor,
                          \$destination=config]);
\end{verbatim}                    

Mientras que la lectura del archivo “modeloBro.log” se realizaría de la siguiente manera:

\begin{verbatim}
Input::add_table([\$source=”modeloBro.log”, \$name=”modeloBro”,
                          \$idx=Word, \$val=Probability,
                          \$destination=Btable]);
\end{verbatim}

Las tablas resultantes luego de leer ambos archivos serían:

\begin{verbatim}
config = {   [Poov1] = 0.0001,
      [Poov2] =  0.0001,
      [Poov3] =  0.0001,
      [Poov4] =  0.0001,
                   [Theta]  =  12.0       };
\end{verbatim}

\begin{verbatim}
Btable = {      [Bss]  =  {192.168.1.13,    1.0}
      [Bsp]  =  {idr,    0.16759},
      [Bsa]  =  {idpd,    0.000006},
      [Bsv]  =  {nombre,    0.000175},
      [numeroPalabraSs]  = { numTotal,    205069.0},
      [numeroPalabraSp]  = {numTotal,    423870.0},
      [numeroPalabraSa]  =  {numTotal,    319727.0},
      [numeroPalabraSv]  =  {numTotal,    319727.0}      };
\end{verbatim}

\subsection{Evaluación de las probabilidades de los URI}

Una vez leídos los parámetros necesarios para que el sistema funcione, el siguiente paso es hallar el índice de anormalidad ($N_{s}$) mediante la expresión \ref{eq:Ns} para de este modo compararlo con el parámetro $\theta$ como se indica en la expresión \ref{eq:ClaseU}. Estas dos tareas esenciales en el módulo de evaluación se dividieron en tres funciones al momento de ser implementadas. Una de las funciones será encargada de calcular tanto el $\varepsilon_{0}$ que aparece en la expresión \ref{eq:sumB} como la sumatoria de los logaritmos de los $p_{qtot}$ que se encuentra en la expresión \ref{eq:Ns}. La segunda función tomará el valor de $\varepsilon_{0}$ y la sumatoria de los logaritmos de los $p_{qtot}$ que calculó la primera función y procederá a efectuar todas las operaciones que hay en la expresión \ref{eq:Ns} para de este modo obtener el índice de anormalidad, $N_{s}$. Por otra parte, la tercera función tendrá como tarea comparar el índice de anormalidad $N_{s}$ con el parámetro $\theta$ y escribir en un archivo de texto aquellos URIs anómalos.

La primera función, fue implementada con la ayuda de las tablas “Btable”, “config” y la estructuras de datos se tipo “uriSegmentado” que fueron explicadas en la secciones \ref{sssec:estructuraModelo} y \ref{sssec:estructuraSegmentacion} respectivamente.  Lo lógica seguida por dicha función fue la siguiente:

Se toma el vocabulario de cada uno de los estado, es decir, la información almacenada en la estructura de tipo “uriSegmentado” y se itera por cada uno de los elementos. Mientras se está realizando la iteración, se verifica si los elementos se encuentran en los datos que almacena la estructura de tipo “Btable”, es decir, en el vector “B” del estado respectivo. Si la palabra se encuentra dentro de la tabla “Btable”, entonces, se procederá a buscar la probabilidad de generación de dicha palabra, si no se encuentra, entonces se utilizará la probabilidad de fuera de vocabulario del estado con el que se está trabajo. Con estos valores se realizarán dos operaciones: La primera será ir sumando cada una de las cantidades en una variable acumuladora para de esta forma calcular  la expresión \ref{eq:sumB} al final de la iteración. Esto se podrá hacer dividiendo el valor que hay en la variable acumuladora entre el número de elementos que posee el vocabulario del estado que se está estudiando. La segunda operación será ir sumando los logaritmos de cada una de las probabilidades e ir almacenando dicha suma en una variable acumuladora, para de este modo calcular la sumatoria de los logaritmos de los $p_{qtot}$ que se encuentra en la expresión \ref{eq:Ns}.

Por otra parte, la segunda función se encargará de tomar los valores calculados por la primera función ($\varepsilon_{0}$ y $\sum log(p_{qtot})$)  así como el campo “número de estados” ( T ) de la estructura de datos de tipo “uriSegmentado”, para luego realizar las operaciones presentes en la ecuación \ref{eq:Ns}.

Finalmente, la tercera verificará si el URI que se está analizando estaba sintácticamente bien construido verificando en el campo “uri correcto” de la estructura de datos de tipo “uriSegmentado”. Si el valor de dicho campo indica que el URI no estuvo bien construido, entonces, se escribirán en un “log”  los datos del URI  con la ayuda de las herramientas que aporta el “Notice Framework” de Bro y la estructura de datos “InfoAtaque” presentada en la sección \ref{sssec:estructuraEvaluacion}. La herramienta de Bro utilizada para realizar la escritura del log fue la función “write” del módulo LOG. La manera en la que se utilizó en el programa fue la siguiente:

\begin{verbatim}
    local rec: InfoAtaque;
    rec = InfoAtaque();

    rec$clasificacion = "Error de sintaxis en el URI : ";
    rec$uri = Segmentacion::parsedUri$uri;
    rec$probability = "-";

    # Se escribe en el archivo
    Log::write(LOG, rec);
\end{verbatim}

Por otra parte, si el valor del campo “uri correcto” indica que el URI está sintácticamente bien construido, entonces, se tomará el índice de anormalidad ($N_{s}$) calculada por la segunda función y siguiendo lo estipulado en la expresión \ref{eq:ClaseU}, se comparará con el parámetro $\theta$ obtenido a través de la tabla “config”. Si el índice de anormalidad es menor o igual que el parámetro $\theta$ entonces, se procederá a escribir en el log de los URIs anómalos de la misma forma en la que se describió anteriormente. La única diferencia sería que en la estructura de datos se escribiría la siguiente información:

\begin{verbatim}
            rec$clasificacion = "Se ha sobrepasado el umbral de anomalia : ";
            rec$uri = Segmentacion::parsedUri$uri;
            rec$probability = cat(" ",indicesAnormalidad);
\end{verbatim}

\section{Implementación del modulo de entrenamiento}

En esta sección se explicarán las estructuras de datos utilizada y el modo en el que se implementó en Bro el módulo de entrenamiento. La explicación teórica del mismo se encuentra detallada en la sección \ref{sec:entrenamiento}.
    
    La implementación del módulo de entrenamiento consta de dos modos, el modo “Offline” y el modo “Online”. El entrenamiento en modo “Offline” se encargará de tomar los segmentos de un flujo de URIs sin anomalías, contar el número de aparición de cada uno de los segmentos de los URIs, para luego calcular la probabilidad de aparición de cada uno de ellos. Una vez culminadas estas tareas se procederá a escribir en un archivo toda la información recolectada, es decir, el conjunto de palabras observadas mientras se hacía el entrenamiento, sus probabilidades de aparición y el estado del autómata en el que se observaron las mismas.

Por otra parte, el modo de entrenamiento en modo “Online” a diferencia del entrenamiento en modo “Offline” necesita como entrada: un flujo de segmentos de URIs y un archivo con un modelo de normalidad ya construido. Esta función, se encargará de ir modificando el modelo de normalidad ingresado a medida que se vayan recibiendo nuevos segmentos de URI para de esta manera construir el nuevo modelo.

\subsection{Estructura de datos}\label{sub:estructuraEntrenamiento}
El módulo de entrenamiento consta de varias estructuras de datos de tipo “register” y de tipo “table” para realizar su trabajo. Tanto el modo “Online” como el “Offline” comparten las mismas estructuras.

La estructuras de tipo “record” serían las siguientes:

“Entrenamiento”, cuyo funcionamiento es ir almacenando el número de veces que una palabra es observada junto con la probabilidad de observación, mientras se realiza el entrenamiento. Los campos de esta estructura son los siguientes:

numPalabras: campo de tipo flotante que almacena el número de veces que una palabra es observada durante el entrenamiento.
probability: campo de tipo flotante que almacene la probabilidad de observación de una palabra.

En Bro, esta estructura luciria de la siguiente forma:

\begin{verbatim}
type Entrenamiento: record { 
        numPalabras : double &default = 1.0;
        probability: double &default = 0.0;
};
\end{verbatim}

La otra estructura de tipo “record” es “Info”. “Info” se encargará de almacenar la información que será escrita en el archivo de texto que representará el modelo de normalidad. Los campos de esta estructura de datos son los siguientes:

\begin{itemize}
\item state: es un campo de tipo “string” que almacenará el nombre del estado al que pertenece la palabra y su probabilidad de observación.
\item word: es un campo de tipo “string” que almacenará las palabras observadas durante el entrenamiento.
\item probability: es un campo de tipo flotante que almacenará la probabilidad de observación de las palabras. 
\end{itemize}
La implementación de la misma se realiza de la siguiente manera haciendo uso del lenguaje de scripting de Bro:

\begin{verbatim}
type Info: record {

        state: string &log &default = "";
        word : string &log &default = "";
        probability: double &log &default = 0.0;
};
\end{verbatim}

Por otra parte, la estructura tipo “table” que se utilizó en la implementación de este módulo fueron las siguientes:

Se hizo uso de una hash table que posee como clave un campo tipo “string” y como valor una estructura de datos de tipo “Entrenamiento”. Esta tabla tendrá como función almacenar las palabras que van apareciendo durante el entrenamiento, el número de veces que fueron observadas las mismas y la probabilidad de aparición . La clave de esta tabla serian las palabras observadas y el resto de la información sería almacenado en la estructura de datos “Entrenamiento”.

Cada estado del autómata tendrá su tabla de entrenamiento propia para de esta manera tener una mayor organización de las observaciones realizadas durante el entrenamiento.

La manera de escribir este tipo de tablas en Bro es la siguiente:

\begin{verbatim}
entrenamientoSs: table[string] of Entrenamiento = table();
entrenamientoSp: table[string] of Entrenamiento = table();
entrenamientoSa: table[string] of Entrenamiento = table();
entrenamientoSv: table[string] of Entrenamiento = table();
\end{verbatim}

\subsection{Entrenamiento Offline}
El entrenamiento en modo “Offline”, como se explicó en la introducción consiste en realizar observaciones de los segmentos de los URIs entrantes y calcular la probabilidad de aparición de cada uno de ellos. La  diferencia fundamental entre este modo y el “Online” es que en el segundo se toma un modelo de normalidad ya construido y se va actualizando a medida que van llegando peticiones nuevas, mientras que el primero se va creando el modelo de normalidad desde cero.

La manera de implementar esta funcionalidad fue dividir la tarea que realiza este modo en dos funciones. La primera función se encargará de ir verificando por estado si los segmentos de los URIs de las peticiones que van llegando se encuentra en las tablas  “entrenamientoSs”, “entrenamientoSp”, “entrenamientoSa” o “entrenamientoSv”  explicadas en la sección \ref{sub:estructuraEntrenamiento} dependiendo el estado al que pertenezca el segmento que se está estudiando. Si  la palabra se encuentra en la tabla correspondiente, entonces se aumentará en una unidad el campo “numPalabras” de la misma. Si no se encuentra, entonces se agregara la palabra a la respectiva tabla de entrenamiento y se se aumentará en una unidad el campo “numPalabras”.

La segunda función se utilizará una vez que el flujo de segmentos en el entrenamiento haya terminado. Esta función se encargará de calcular la probabilidad de aparición de las palabras dada una tabla del tipo de “entrenamientoSs”, “entrenamientoSp”, “entrenamientoSa” o “entrenamientoSv” y el número de palabras total de la misma haciendo uso de la fórmula (3.13 agregar referencia).

\subsection{Entrenamiento Online}\label{sec:entrenamientoOffline}

El modo de entrenamiento “Online” funciona de una manera un poco diferente al modo “Offline” ya que en este modo de entrenamiento existe un modelo de normalidad previamente construido que se tendrá que ir modificando a medida que van llegando nuevos flujos de segmentos de URI. El trabajo que se debe realizar en este modo fue dividido en dos funciones:

La primera función será la encargada de leer el modelo de normalidad de la misma forma en que se explicó en la sección \ref{sec:lecturaModelo}, es decir, se creará una tabla como la “Btable” que contendrá toda la información del archivo de texto y además, se extraerá  el número total de palabras observadas por estado que se encuentra en el modelo. Estos valores serán almacenados en varias variables para ser utilizados posteriormente.

La segunda función verificará si las palabras entrantes se encuentran en el conjunto de palabras leídas previamente en el modelo de normalidad. Si no se encuentran, entonces, se introducirá la nueva palabra a la tabla creada por la primera función ( “Btable”) y se calculará la probabilidad de aparición de la misma mediante el uso de la fórmula \ref{eq:entrenamiento}. 

Por otra parte, si la palabra existe dentro de la tabla “Btable”, entonces, se procederá a hallar el número de veces que esta fue observada anteriormente para calcular la nueva probabilidad de aparición. Como se tiene la probabilidad antigua de aparición de dicha palabra ($b_{ij}$) y el número total de palabras observadas en el estado ($\sum_{s=1}^{L}\sum_{t=1}^{Ts}\delta(q_{t}^{s} = s_{i})$), entonces, se puede despejar la fórmula \ref{eq:entrenamiento} utilizada para calcular la probabilidad de aparición de la siguiente forma:

\begin{equation}\label{eq:probAparicion}
b_{ij}*(\sum_{s=1}^{L}\sum_{t=1}^{Ts}\delta(q_{t}^{s} = s_{i}) ) = \sum_{s=1}^{L}\sum_{t=1}^{Ts}\delta(o_{t}^{s} = v_{i,j} ,q_{t}^{s} = s_{i} )      
\end{equation}

Una vez calculado el número de veces que la palabra fue observada anteriormente con la ayuda de la fórmula \ref{eq:probAparicion}, se puede hallar la nueva probabilidad de aparición de la misma haciendo uso de la fórmula \ref{eq:entrenamiento}.

En Bro este procedimiento se escribiría de la siguiente manera:

\begin{verbatim}
# Se calcula el número de veces que la palabra fue observada anteriormente
x =  numPalabras*Btable[state,word]$probability;

# Se suma una unidad al número de palabras total y se calcula la nueva probabilidad de #aparicion.
numPalabras = numPalabras + 1;
Btable[state,word]$probability = (x + 1)/(numPalabras);
\end{verbatim}

Donde :
\begin{itemize}
\item numPalabras es el número total de palabras observadas en un estado.
\item state es el nombre del estado al que pertenece la palabra que se está estudiando.
\item word contiene la palabra que se está estudiando.
\item Btable es la tabla que contiene la información del modelo de normalidad leído en la primera función descrita en esta sección. El tipo de dato de “Btable” está mostrado en la sección (introducir número de sección en donde se explica el Btable)
\end{itemize}

\subsection{Escritura del modelo de normalidad en Bro}

    La escritura del modelo de normalidad que se implementó a través de dos funciones. Una para escribir el archivo de salida cuando se esté entrenando en modo “Online” y otra para cuando se entrene en modo “Offline”. Estas funciones se encargarán de tomar los datos recolectados en el entrenamiento y escribirlos en un archivo de texto siguiendo el formato establecido en la sección \ref{sec:lecturaModelo}.

    Para realizar esta tarea se hizo uso de la estructura de datos “Info”, que fue explicada en la sección \ref{sub:estructuraEntrenamiento} y de la herramienta “Logging Framework” de Bro. En concreto, se utilizaron las funciones “create\_stream” para crear el archivo en donde se va a escribir la información y la función “write”  para escribir los datos sobre el mismo.

    La manera en que se utilizaron las herramientas junto con la estructura de datos “Info” fue la siguiente:

Si el entrenamiento se está realizando en modo “Online”, entonces la función que se encarga de escribir el modelo tendrá la siguiente implementación:

\begin{verbatim}
    # Se crea el archivo.
    local nombreArchivo = "modeloBro";
    Log::create_stream(LOG, [$columns=Info, $path=nombreArchivo]);

    # Se inicializa el registro que se utilizará para escribir sobre el
    # archivo.
    local rec: Info;
    rec = Info();

    # Se itera sobre las palabras del vocabulario para guardarlas en el log.
    for ([estado,palabra] in Btable){

        rec$word = palabra;
        rec$probability = vocabulario[estado,palabra]$probability;
        rec$state = estado;

        # Se escribe en el archivo
        Log::write(LOG, rec);
        
    }
\end{verbatim}

Por otra parte, si el modo de entrenamiento es en modo “Offline”, entonces la función que se encargará de escribir el archivo de texto estará implementada de la siguiente manera:

\begin{verbatim}
    # Se crea el archivo.
    local nombreArchivo = "modeloBro";
    Log::create_stream(LOG, [$columns=Info, $path=nombreArchivo]);

    # Se inicializa el registro que se utilizará para escribir sobre el
    # archivo.
    local rec: Info;
    rec = Info();

    for (i in vocabulario){

        # Se itera sobre las palabras del vocabulario para guardarlas en el log.
        for (palabra in vocabulario[i]){

            rec$word = palabra;
            rec$probability = vocabulario[i][palabra]$probability;
            rec$state = tablaEstados[i];

            # Se escribe en el archivo
            Log::write(LOG, rec);
            
        }

    }
\end{verbatim}
Donde :

\begin{itemize}
\item vocabulario serán las tablas “entrenamientoSs”, “entrenamientoSp”, “entrenamientoSa” y “entrenamientoSv” explicadas en la sección (introducir numero de la sección).
\end{itemize}

Es preciso recalcar que la función debe recibir cada una de las tablas de entrenamiento de cada uno de los estados.
